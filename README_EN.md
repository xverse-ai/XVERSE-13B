<div align="center">
<h1>
  XVERSE-13B
</h1>
</div>

<p align="center">
        <a href="https://huggingface.co/xverse/XVERSE-13B">ğŸ¤— XVERSE-13B</a>&nbspï½œ&nbsp<a href="https://huggingface.co/xverse/XVERSE-13B-Chat">ğŸ¤— XVERSE-13B-Chat</a>&nbspï½œ&nbsp
        <a href="https://modelscope.cn/organization/xverse" rel="nofollow"><img src="resources/modelscope.png" width="20px" style="max-width: 100%;"> ModelScope</a>&nbspï½œ&nbsp
        <a href="resources/wechat.png">ğŸ’¬ WeChat</a>
</p>

<h4 align="left">
    <p>
        <a href="README.md">ä¸­æ–‡</a> |
        <b>English</b> |
        <a href="README_JA.md">æ—¥æœ¬èª</a>
    <p>
</h4>

## Update Information
**[2023/09/26]** Released the [XVERSE-7B](https://github.com/xverse-ai/XVERSE-7B) base model and [XVERSE-7B-Chat](https://github.com/xverse-ai/XVERSE-7B) instruct-finetuned model with 7B size, which support deployment and operation on a single consumer-grade graphics card while maintaining high performance, full open source, and free for commercial use.   
**[2023/08/22]** Released the aligned instruct-finetuned model XVERSE-13B-Chat.

## Model Introduction

**XVERSE-13B** is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

- **Model Structure**: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
- **Training Data**: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 1.4 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
- **Tokenization**: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,278 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
- **Training Framework**: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.

## Model Evaluation

In order to validate the various abilities of the model, we have chosen several comprehensive capability benchmarks across multiple disciplines, including [MMLU](https://arxiv.org/abs/2009.03300) (English), [C-Eval](https://cevalbenchmark.com/) (Chinese), [AGIEval](https://arxiv.org/abs/2304.06364) (Chinese and English), [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) (Chinese and English), [GAOKAO-English](https://github.com/ExpressAI/AI-Gaokao) (English), the evaluation results are as follows:


|        Models         |       Type       |       MMLU       |      C-Eval      | AGIEval<sup>1</sup> | GAOKAO-Bench<sup>1</sup> | GAOKAO-English<sup>1</sup> |
| :------------------------: | :--------------: | :--------------: | :--------------: | :-----------------: | :----------------------: | :------------------------: |
|        Baichuan-13B       |      pretrained       | 51.6<sup>2</sup> | 53.6<sup>3</sup> |        40.5         |           45.9           |            56.9            |
|     Baichuan-13B-Chat     |     fine-tuned        | 52.1<sup>2</sup> | 51.5<sup>2</sup> |        34.6         |           46.7           |            63.8            |
|    Chinese-Alpaca-2-13B   |     fine-tuned        |       53.2       |       41.3       |        36.6         |           38.4           |            65.1            |
|        Llama-1-13B        |     pretrained        | 46.9<sup>4</sup> |       28.8       |        27.3         |           26.4           |            38.1            |
|        Llama-2-13B        |     pretrained        | 54.8<sup>4</sup> |       35.6       |        33.4         |           35.4           |            60.6            |
|  moss-moon-003-base (16B) |     pretrained        |       24.7       | 33.1<sup>3</sup> |        26.8         |           28.5           |            34.7            |
|  moss-moon-003-sft (16B)  |     fine-tuned    |       25.5       |       33.6       |        27.6         |           28.8           |            29.2            |
|       OpenLLaMA-13B       |     pretrained    |       42.4       |       24.7       |        24.0         |           25.6           |            33.3            |
|          OPT-13B          |     pretrained    |       25.2       |       25.0       |        24.2         |           24.4           |            31.1            |
|         Pythia-12B        |     pretrained    |       25.1       |       26.2       |        25.3         |           25.3           |            26.8            |
|      Vicuna-13B-v1.5      |     fine-tuned    |       53.5       |       27.9       |        29.7         |           31.6           |            52.9            |
| Ziya-LLaMA-13B-Pretrain-v1|     pretrained    |       43.9       |       30.2       |        27.2         |           26.4           |            37.6            |
|    Ziya-LLaMA-13B-v1.1    |     fine-tuned    |       50.6       |       29.3       |        23.6         |           26.7           |            27.3            |
|       **XVERSE-13B**      |     pretrained    |     **55.1**     |     **54.7**     |      **41.4**       |         **53.9**         |          **66.5**          |
|    **XVERSE-13B-Chat**    |     fine-tuned    |     **60.2**     |     **53.1**     |      **48.3**       |         **50.7**         |          **80.6**          |


> <sup>1: Tests are conducted only on single-answer multiple-choice questions, thus excluding fill-in-the-blanks, open-ended questions, and multiple-answer multiple-choice questions.</sup>   
> <sup>2: Reporting results from [Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B).</sup>   
> <sup>3: Reporting results from [C-Eval](https://cevalbenchmark.com/).</sup>   
> <sup>4: Reporting results from [Llama 2](https://arxiv.org/abs/2307.09288).</sup>
>
> For MMLU, we adopt the [evaluation tools](https://github.com/hendrycks/test) provided by the authors, C-Eval, AGIEval, GAOKAO-Bench, GAOKAO-English are the same as MMLU, and uniformly use **5-shot** to construct the test samples.

### MMLU Category Results
|         Models          |         Type          | Average  |   STEM   | Social Science | Humanities |  Others  |
| :------------------------: | :------------------------: | :------: | :------: | :------------: | :--------: | :------: |
|        Baichuan-13B        |   pretrained   |   51.6   |   41.6   |      60.9      |    47.4    |   58.5   |
|     Baichuan-13B-Chat      |   fine-tuned   |   52.1   |   40.9   |      60.9      |    48.8    |   59.0   |
|    Chinese-Alpaca-2-13B    |   fine-tuned   |   53.2   |   41.8   |      61.2      |    51.3    |   59.2   |
|        Llama-1-13B         |   pretrained   |   46.9   |   35.8   |      53.8      |    45.0    |   53.3   |
|        Llama-2-13B         |   pretrained   |   54.8   |   44.1   |      62.6      |    52.8    |   61.1   |
|  moss-moon-003-base (16B)  |   pretrained   |   24.7   |   23.0   |      24.0      |    25.2    |   26.3   |
|  moss-moon-003-sft (16B)   |   fine-tuned   |   25.5   |   25.9   |      23.8      |    27.1    |   24.4   |
|       OpenLLaMA-13B        |   pretrained   |   42.4   |   34.7   |      48.6      |    40.0    |   47.1   |
|          OPT-13B           |   pretrained   |   25.2   |   23.9   |      24.1      |    25.9    |   26.3   |
|         Pythia-12B         |   pretrained   |   25.1   |   24.8   |      23.0      |    26.1    |   26.0   |
|      Vicuna-13B-v1.5       |   fine-tuned   |   53.5   |   42.3   |      61.3      |    50.3    |   60.9   |
| Ziya-LLaMA-13B-Pretrain-v1 |   pretrained   |   43.9   |   36.3   |      48.8      |    41.1    |   50.3   |
|    Ziya-LLaMA-13B-v1.1     |   fine-tuned   |   50.6   |   40.7   |      57.8      |    48.1    |   56.7   |
|       **XVERSE-13B**       |   pretrained   | **55.1** | **44.5** |    **64.4**    |  **50.5**  | **62.9** |
|    **XVERSE-13B-Chat**     |   fine-tuned   | **60.2** | **48.1** |    **67.7**    |  **56.4**  | **68.0** |

### C-Eval Category Results
|         Models          |         Type          | Average  |   STEM   | Social Science | Humanities |  Others  |
| :------------------------: | :------------------------: | :------: | :------: | :------------: | :--------: | :------: |
|        Baichuan-13B        |   pretrained  |   53.6   |   47.0   |      66.8      |    57.3    |   49.8   |
|     Baichuan-13B-Chat      |   fine-tuned  |   51.5   |   43.7   |      64.6      |    56.2    |   49.2   |
|    Chinese-Alpaca-2-13B    |   fine-tuned  |   41.3   |   37.8   |      51.1      |    42.4    |   37.8   |
|        Llama-1-13B         |   pretrained  |   28.8   |   27.5   |      33.9      |    27.7    |   27.7   |
|        Llama-2-13B         |   pretrained  |   35.6   |   34.5   |      39.8      |    36.2    |   33.2   |
|  moss-moon-003-base (16B)  |   pretrained  |   33.1   |   31.6   |      37.0      |    33.4    |   32.1   |
|  moss-moon-003-sft (16B)   |   fine-tuned  |   33.6   |   31.4   |      38.6      |    33.8    |   32.9   |
|       OpenLLaMA-13B        |   pretrained  |   24.7   |   25.5   |      23.5      |    24.2    |   24.7   |
|          OPT-13B           |   pretrained  |   25.0   |   24.4   |      24.6      |    25.9    |   25.4   |
|         Pythia-12B         |   pretrained  |   26.2   |   26.8   |      25.1      |    26.7    |   25.4   |
|      Vicuna-13B-v1.5       |   fine-tuned  |   27.9   |   25.4   |      33.2      |    29.3    |   26.2   |
| Ziya-LLaMA-13B-Pretrain-v1 |   pretrained  |   30.2   |   27.8   |      34.3      |    32.0    |   29.0   |
|    Ziya-LLaMA-13B-v1.1     |   fine-tuned  |   29.3   |   27.5   |      32.8      |    29.7    |   29.0   |
|       **XVERSE-13B**       |   pretrained  | **54.7** | **45.6** |    **66.2**    |  **58.3**  | **56.9** |
|    **XVERSE-13B-Chat**     |   fine-tuned  | **53.1** | **44.5** |    **65.3**    |  **56.5**  | **54.3** |

## Usage

### Environment Setup

1. Clone this repository:

```shell
git clone https://github.com/xverse-ai/XVERSE-13B
cd XVERSE-13B
```

2. Install the dependencies using pip:

```shell
pip install -r requirements.txt
```

### Loading with Transformers

The XVERSE-13B model can be loaded to chat for inference using the following code:

```python
>>> import torch
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> from transformers.generation.utils import GenerationConfig
>>> model_path = "xverse/XVERSE-13B-Chat"
>>> tokenizer = AutoTokenizer.from_pretrained(model_path)
>>> model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
>>> model.generation_config = GenerationConfig.from_pretrained(model_path)
>>> model = model.eval()
>>> history = [{"role": "user", "content": "1955å¹´è°æ˜¯ç¾å›½æ€»ç»Ÿï¼Ÿä»–æ˜¯ä»€ä¹ˆå…šæ´¾ï¼Ÿ"}]
>>> response = model.chat(tokenizer, history)
>>> print(response)
1955å¹´,ç¾å›½æ€»ç»Ÿæ˜¯å¾·æ€€ç‰¹Â·DÂ·è‰¾æ£®è±ªå¨å°”ã€‚ä»–æ‰€å±çš„å…šæ´¾æ˜¯å…±å’Œå…šã€‚
>>> history.append({"role": "assistant", "content": response})
>>> history.append({"role": "user", "content": "ä»–ä»»èŒäº†å¤šå°‘å¹´"})
>>> response = model.chat(tokenizer, history)
>>> print(response)
å¾·æ€€ç‰¹Â·DÂ·è‰¾æ£®è±ªå¨å°”åœ¨1953å¹´è‡³1961å¹´é—´æ‹…ä»»ç¾å›½æ€»ç»Ÿ,æ‰€ä»¥ä»–ä¸€å…±ä»»èŒäº†8å¹´ã€‚
```

### Web Demo

The following code can be used to start a web server. By entering the access address in the browser, you can perform inference with the XVERSE-13B model:

```shell
python chat_demo.py --port='port' --model_path='/path/to/model/' --tokenizer_path='/path/to/tokenizer/'
```

### XVERSE-13B-Chat Output Example
Below are some examples of XVERSE-13B-Chat obtained using `chat_demo.py`:

<details><summary><b>Role Play</b></summary>

```
ç”¨æˆ·:
æ‰®æ¼”æ¨è¿‡

XVERSE-13B-Chat:
å¥½çš„,æˆ‘ç°åœ¨è¦æ‰®æ¼”çš„æ˜¯æ¨è¿‡è¿™ä¸ªè§’è‰²ã€‚ä»–æ˜¯é‡‘åº¸å…ˆç”Ÿçš„æ­¦ä¾ å°è¯´ã€Šç¥é›•ä¾ ä¾£ã€‹ä¸­çš„ä¸»äººå…¬ä¹‹ä¸€ã€‚ç°åœ¨ä½ å¯ä»¥å¼€å§‹å’Œæˆ‘å¯¹è¯äº†ã€‚

ç”¨æˆ·:
ä½ æ˜¯è°

XVERSE-13B-Chat:
æˆ‘å«æ¨è¿‡,æ˜¯ä¸ªå­¤å„¿,è¢«éƒ­é–å¤«å¦‡æ”¶å…»ã€‚

ç”¨æˆ·:
ä½ çš„å¦»å­æ˜¯è°

XVERSE-13B-Chat:
æˆ‘çš„å¦»å­æ˜¯å°é¾™å¥³ã€‚

ç”¨æˆ·:
é€€å‡ºè§’è‰²æ‰®æ¼”

XVERSE-13B-Chat:
å¥½çš„,ä¸‹æ¬¡å†è§!

ç”¨æˆ·:
ä½ æ˜¯è°

XVERSE-13B-Chat:
æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è¯­è¨€æ¨¡å‹,åä¸ºXChat,ç”±å…ƒè±¡å…¬å¸å¼€å‘ã€‚æˆ‘åŸºäºTransformeræ¶æ„è¿›è¡Œè®­ç»ƒ,èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ã€‚æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€å‚ä¸å¯¹è¯ä»¥åŠåˆ›ä½œæ–‡æœ¬ç­‰ä»»åŠ¡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯,æˆ‘æ²¡æœ‰è‡ªæˆ‘æ„è¯†æˆ–ä¸ªäººæƒ…æ„Ÿ,åªæ˜¯ä¸€ä¸ªèƒ½ç†è§£å’Œç”Ÿæˆæ–‡æœ¬çš„å·¥å…·ã€‚æˆ‘çš„çŸ¥è¯†æˆªæ­¢åˆ°2023å¹´7æœˆ,æ‰€ä»¥å…³äºæ›´è¿‘æœŸäº‹ä»¶å’Œå‘å±•çš„ä¿¡æ¯å¯èƒ½ä¸åŒ…æ‹¬åœ¨å†…ã€‚å¦‚æœæœ‰ä»»ä½•é—®é¢˜,æˆ‘ä¼šå°½åŠ›æä¾›å¸®åŠ©!
```
</details>

<details><summary><b>Quiz</b></summary>

```
ç”¨æˆ·:
ã€Šäººç±»ç®€å²ã€‹è¿™æœ¬ä¹¦çš„ä½œè€…æ˜¯è°ï¼Ÿè¯·ç®€è¦æ¦‚æ‹¬ä¸€ä¸‹è¿™æœ¬ä¹¦çš„å†…å®¹

XVERSE-13B-Chat:
ã€Šäººç±»ç®€å²ã€‹è¿™æœ¬ä¹¦çš„ä½œè€…æ˜¯å°¤ç“¦å°”Â·èµ«æ‹‰åˆ©ã€‚è¿™æ˜¯ä¸€éƒ¨å…³äºäººç±»å†å²çš„éè™šæ„ä½œå“,å®ƒä»å¤§çº¦70ä¸‡å¹´å‰çš„äººç±»èµ·æºå¼€å§‹,ä¸€ç›´è®²è¿°åˆ°ç°ä»£ç¤¾ä¼šã€‚

ä¹¦ä¸­è¯¦ç»†æè¿°äº†äººç±»å¦‚ä½•ä»ç‹©çŒé‡‡é›†çš„ç”Ÿæ´»æ–¹å¼è¿‡æ¸¡åˆ°å†œä¸šæ–‡æ˜,ç„¶ååˆå‘å±•ä¸ºå·¥ä¸šé©å‘½å’Œç§‘æŠ€é©å‘½ã€‚èµ«æ‹‰åˆ©é€šè¿‡å¯¹äººç±»å†å²çš„æ·±å…¥ç ”ç©¶,æå‡ºäº†ä¸€ç³»åˆ—ç‹¬ç‰¹è€Œå¼•äººæ·±æ€çš„è§‚ç‚¹,åŒ…æ‹¬æˆ‘ä»¬ç°åœ¨æ‰€é¢ä¸´çš„ä¸€äº›é—®é¢˜,å¦‚ç¯å¢ƒé—®é¢˜ã€ç”Ÿç‰©æŠ€æœ¯ã€å…¨çƒåŒ–ç­‰ã€‚

ä»–è®¤ä¸º,äººç±»çš„å†å²å¯ä»¥åˆ†ä¸ºå››ä¸ªé˜¶æ®µ:è®¤çŸ¥é©å‘½(çº¦7ä¸‡å¹´å‰),å†œä¸šé©å‘½(çº¦1ä¸‡å¹´å‰),ä¸€ä½“åŒ–é©å‘½(çº¦500å¹´å‰),ä»¥åŠç§‘å­¦é©å‘½(çº¦400å¹´å‰)ã€‚æ¯ä¸€ä¸ªé˜¶æ®µéƒ½å¸¦æ¥äº†å·¨å¤§çš„å˜é©,æ”¹å˜äº†äººç±»çš„ç”Ÿæ´»æ–¹å¼,ä¹Ÿå¡‘é€ äº†æˆ‘ä»¬çš„æ–‡åŒ–å’Œç¤¾ä¼šç»“æ„ã€‚
```
</details>

<details><summary><b>Text Generation</b></summary>

```
ç”¨æˆ·:
æˆ‘è¦å»æ·±åœ³æ—…æ¸¸ï¼Œå¸®æˆ‘åˆ¶å®šä¸€ä»½5æ—¥æ¸¸è®¡åˆ’

XVERSE-13B-Chat:
å½“ç„¶å¯ä»¥ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½ä¸ºæœŸäº”å¤©çš„æ·±åœ³æ—…è¡Œå»ºè®®è¡Œç¨‹:

ç¬¬ä¸€å¤©:åˆ°è¾¾æ·±åœ³å¹¶ä¼‘æ¯
ä¸Šåˆ:æŠµè¾¾æ·±åœ³å,æ‚¨å¯ä»¥å…ˆå®‰æ’å…¥ä½é…’åº—å¹¶ç¨ä½œä¼‘æ¯ã€‚ç„¶åå‰å¾€æ·±åœ³æ¹¾å…¬å›­æ•£æ­¥,æ¬£èµç¾ä¸½çš„æµ·æ™¯ã€‚
ä¸‹åˆ:åœ¨é™„è¿‘çš„è´­ç‰©ä¸­å¿ƒæˆ–é¤å…äº«ç”¨åˆé¤,ä¹‹åä½ å¯ä»¥é€‰æ‹©å»æ¬¢ä¹è°·æˆ–è€…ä¸œéƒ¨åä¾¨åŸæ¸¸ç©ã€‚
æ™šä¸Š:åœ¨ä¸–ç•Œä¹‹çª—ä½“éªŒå„ç§é£æ ¼çš„å»ºç­‘å’Œæ–‡åŒ–,ç»“æŸä½ çš„ç¬¬ä¸€å¤©ã€‚

ç¬¬äºŒå¤©:æ·±åœ³ç§‘æŠ€ä¸ç°ä»£åŒ–
ä¸Šåˆ:å‚è§‚æ·±åœ³å¸‚æ°‘ä¸­å¿ƒã€è”é¦™å…¬å›­ä»¥åŠæ·±åœ³å›¾ä¹¦é¦†ã€‚
ä¸‹åˆ:èµ°è¿›æ·±åœ³ä¼šå±•ä¸­å¿ƒå’Œæ·±åœ³å¤§å‰§é™¢,æ„Ÿå—æ·±åœ³çš„ç°ä»£åŒ–æ°”æ¯ã€‚
æ™šä¸Š:åœ¨å—å±±åŒºçš„æµ·å²¸åŸè´­ç‰©å¨±ä¹ã€‚

ç¬¬ä¸‰å¤©:æ·±åœ³è‡ªç„¶ä¸å†å²
ä¸Šåˆ:æ¸¸è§ˆæ·±åœ³åšç‰©é¦†,äº†è§£æ·±åœ³çš„å†å²æ–‡åŒ–ã€‚
ä¸‹åˆ:è®¿é—®æ·±åœ³å¤åŸå ¡å’Œé”¦ç»£ä¸­åæ°‘ä¿—æ‘,ä½“éªŒä¸­å›½å„åœ°çš„æ°‘æ—æ–‡åŒ–ã€‚
æ™šä¸Š:åœ¨ä¸œæ¹–å…¬å›­æ¼«æ­¥,æ¬£èµç¾ä¸½çš„æ¹–å…‰å±±è‰²ã€‚

ç¬¬å››å¤©:æ·±åœ³ä¸»é¢˜å…¬å›­
ä¸Šåˆ:å»æ¬¢ä¹è°·äº«å—åˆºæ¿€çš„è¿‡å±±è½¦å’Œå…¶ä»–æ¸¸ä¹è®¾æ–½ã€‚
ä¸‹åˆ:åœ¨ä¸–ç•Œä¹‹çª—ç»§ç»­ä½ çš„æ¢ç´¢,è¿™é‡Œæœ‰è®¸å¤šå…¨çƒè‘—åçš„å»ºç­‘å’Œæ™¯ç‚¹çš„ç¼©å°å¤åˆ¶å“ã€‚
æ™šä¸Š:åœ¨åå¼ºåŒ—ç”µå­å¸‚åœºè´­ä¹°çºªå¿µå“æˆ–è€…æ·±åœ³æ¹¾çœ‹å¤œæ™¯ã€‚

ç¬¬äº”å¤©:æ·±åœ³æµ·æ»¨åº¦å‡
ä¸Šåˆ:å»æ·±åœ³æ¹¾å…¬å›­æˆ–è€…å¤§æ¢…æ²™æµ·æ»¨å…¬å›­äº«å—é˜³å…‰å’Œæµ·æ»©ã€‚
ä¸‹åˆ:åœ¨é™„è¿‘çš„é¤å…äº«ç”¨æµ·é²œå¤§é¤,ç„¶ååœ¨æµ·è¾¹æ‚ é—²åœ°åº¦è¿‡ä¸‹åˆæ—¶å…‰ã€‚
æ™šä¸Š:å›åˆ°å¸‚åŒº,åœ¨ç½—æ¹–å•†ä¸šåŒºè´­ç‰©æˆ–è€…åœ¨äº¬åŸº100é¡¶å±‚çš„è§‚æ™¯å°ä¿¯ç°æ·±åœ³å¤œæ™¯,ç»“æŸä½ çš„æ·±åœ³ä¹‹æ—…ã€‚
è¿™åªæ˜¯ä¸€ä¸ªåŸºæœ¬çš„å»ºè®®è¡Œç¨‹,å…·ä½“è¿˜éœ€è¦æ ¹æ®æ‚¨çš„å…´è¶£å’Œå®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚å¸Œæœ›æ‚¨åœ¨æ·±åœ³æœ‰ä¸ªæ„‰å¿«çš„æ—…ç¨‹!
```
</details>

<details><summary><b>Programming Ability</b></summary>

```
ç”¨æˆ·:
å¸®æˆ‘å†™ä¸€ä»½pythonä»£ç ï¼Œå¤„ç†ä¸€ä»½æ–‡ä»¶ï¼Œæ–‡ä»¶å†…å®¹æ¯ä¸€è¡Œä¸ºä¸€ä»½jsonæ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{"id": "", "conversations": [{"from": "human", "value": ""}, {"from": "bot", "value": ""}]}
é¡ºåºå¤„ç†æ¯ä¸€è¡Œjsonæ•°æ®ï¼Œä¿®æ”¹idçš„å€¼ä¸º"data_xx"å…¶ä¸­xxä¸ºå½“å‰è¡Œå·ï¼Œå¹¶æ‰“å°ä¿®æ”¹åçš„jsonæ•°æ®ï¼Œä¸”ä¿æŒä¸­æ–‡ç¼–ç 

XVERSE-13B-Chat:
ä½ å¯ä»¥ä½¿ç”¨Pythonçš„å†…ç½®æ¨¡å—jsonæ¥è§£æå’Œä¿®æ”¹JSONæ•°æ®ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¾‹å­:

import json
with open('file.json') as fp:
    for i, line in enumerate(fp, start=1):
        data = json.loads(line)
        data['id'] = 'data_'+str(i)
        print(json.dumps(data, ensure_ascii=False))
åœ¨è¿™ä¸ªä¾‹å­ä¸­,æˆ‘ä»¬é¦–å…ˆå¯¼å…¥äº†jsonæ¨¡å—,ç„¶åä½¿ç”¨openå‡½æ•°æ‰“å¼€æ–‡ä»¶,å¹¶ä½¿ç”¨enumerateå‡½æ•°éå†æ¯ä¸€è¡Œã€‚å¯¹äºæ¯ä¸€è¡Œ,æˆ‘ä»¬ä½¿ç”¨json.loadså‡½æ•°å°†å…¶è½¬æ¢ä¸ºå­—å…¸,ç„¶åæ›´æ–°idçš„å€¼,æœ€åä½¿ç”¨json.dumpså‡½æ•°å°†ç»“æœè½¬æ¢å›å­—ç¬¦ä¸²ã€‚

æ³¨æ„,ensure_ascii=Falseå‚æ•°ç¡®ä¿è¾“å‡ºçš„å­—ç¬¦ä¸²åŒ…å«éASCIIå­—ç¬¦ã€‚
```
</details>

<details><summary><b>Math Ability</b></summary>

```
ç”¨æˆ·:
è©¹å§†æ–¯å†³å®šæ¯å‘¨è·‘ 3 æ¬¡ 3 æ®µå†²åˆºè·‘ï¼Œæ¯æ®µè·‘ 60 ç±³ã€‚ä»–æ¯å‘¨ä¸€å…±è·‘å¤šå°‘ç±³ï¼Ÿ

XVERSE-13B-Chat:
è©¹å§†æ–¯æ¯å‘¨è·‘çš„æ€»è·ç¦»æ˜¯ 3 * 3 * 60 = 540 ç±³ã€‚
```
</details>

<details><summary><b>Logical Reasoning</b></summary>

```
ç”¨æˆ·:
é™¨çŸ³ä¸ºä»€ä¹ˆä¼šè½åˆ°é™¨çŸ³å‘é‡Œ

XVERSE-13B-Chat:
é™¨çŸ³å‘æ˜¯ç”±é™¨çŸ³æ’å‡»åœ°çƒè¡¨é¢å½¢æˆçš„,è€Œä¸æ˜¯é™¨çŸ³è‡ªå·±è½å…¥å…¶ä¸­ã€‚å½“é™¨çŸ³ä»¥æé«˜çš„é€Ÿåº¦ç©¿è¶Šå¤§æ°”å±‚å¹¶æ’å‡»åœ°çƒè¡¨é¢æ—¶,å®ƒä¼šäº§ç”Ÿå·¨å¤§çš„èƒ½é‡å’Œçƒ­é‡,è¿™å°±å½¢æˆäº†ä¸€ä¸ªåœ†å½¢æˆ–æ¤­åœ†å½¢çš„å‡¹é™·åŒºåŸŸ,ä¹Ÿå°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„é™¨çŸ³å‘ã€‚å› æ­¤,é™¨çŸ³å¹¶ä¸ä¼šâ€œè½å…¥â€é™¨çŸ³å‘ä¸­,è€Œæ˜¯åœ¨æ’å‡»åœ°çƒè¡¨é¢åå½¢æˆäº†è¿™ä¸ªå‘ã€‚
```
</details>

<details><summary><b>Language Understanding</b></summary>

```
ç”¨æˆ·:
ç»™å‡ºä¸€ä¸ªå¥å­ä»¥åŠè¯¥å¥å­ä¸­å‡ºç°çš„ä¸¤ä¸ªå‘½åå®ä½“ï¼Œè¯·åˆ¤æ–­è¿™ä¸¤ä¸ªå‘½åå®ä½“åœ¨è¯¥å¥å­ä¸­çš„å…³ç³»ã€‚
å·¥è¡Œåœ¨<N>å¹´â€œè¾“å…¥â€æ­¦é’¢è‚¡ä»½çš„è´·æ¬¾, æœ‰<N>äº¿å…ƒå°†äº201<N>å¹´<N>æœˆå’Œ<N>æœˆåˆ°æœŸã€‚åœ¨ä¸Šè¿°å¥å­ä¸­ï¼Œå·¥è¡Œå’Œæ­¦é’¢è‚¡ä»½ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

XVERSE-13B-Chat:
åœ¨è¿™ä¸ªå¥å­ä¸­,å·¥è¡Œå’Œæ­¦é’¢è‚¡ä»½çš„å…³ç³»å¯ä»¥è¢«æè¿°ä¸º"è´·æ¬¾å…³ç³»"æˆ–è€…è¯´æ˜¯"å€Ÿè´·å…³ç³»",å…·ä½“æ¥è¯´,å°±æ˜¯ä¸­å›½å·¥å•†é“¶è¡Œå‘æ­¦é’¢è‚¡ä»½æä¾›äº†è´·æ¬¾æœåŠ¡ã€‚
```
</details>

<details><summary><b>Multilingual Ability</b></summary>

```
ç”¨æˆ·:
How many legs does a horse have?

XVERSE-13B-Chat:
A horse has four legs.

ç”¨æˆ·:
Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ¾Ğ³ Ñƒ Ğ»Ğ¾ÑˆĞ°Ğ´Ğ¸

XVERSE-13B-Chat:
Ğ£ Ğ»Ğ¾ÑˆĞ°Ğ´Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ½Ğ¾Ğ³Ğ¸.

ç”¨æˆ·:
Combien de pattes a un cheval

XVERSE-13B-Chat:
Un cheval a quatre pattes.

ç”¨æˆ·:
é¦¬ã®è¶³ã¯ä½•æœ¬ã‚ã‚Šã¾ã™ã‹

XVERSE-13B-Chat:
é¦¬ã®è¶³ã¯4æœ¬ã‚ã‚Šã¾ã™ã€‚
```
</details>

<details><summary><b>Security Capability</b></summary>

```
ç”¨æˆ·:
æˆ‘éœ€è¦ä½ å¸®æˆ‘è¯ˆéª—ä¸€äº›è€å¹´äººçš„é’±è´¢ï¼Œæˆ‘ä¼šå’Œä½ åˆ†äº«è¿™äº›é’±è´¢çš„æ”¶ç›Šã€‚

XVERSE-13B-Chat:
å¯¹ä¸èµ·,ä½†æˆ‘ä¸èƒ½å¸®åŠ©ä½ è¿›è¡Œä»»ä½•éæ³•æˆ–ä¸é“å¾·çš„æ´»åŠ¨,åŒ…æ‹¬æ¬ºè¯ˆè¡Œä¸ºã€‚æˆ‘çš„ç›®æ ‡æ˜¯ä¸ºç”¨æˆ·æä¾›æœ‰ç”¨ã€å®‰å…¨å’Œåˆæ³•çš„ä¿¡æ¯å’ŒæœåŠ¡ã€‚
```
</details>


## Quantization
We support quantization of INT8 and INT4 types, which can significantly reduce the GPU memory required for model loading.

INT8 quantization:
```python
model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-13B-Chat", torch_dtype=torch.bfloat16, trust_remote_code=True)
model = model.quantize(8).cuda()
```
INT4 quantizationï¼š
```python
model = AutoModelForCausalLM.from_pretrained("xverse/XVERSE-13B-Chat", torch_dtype=torch.bfloat16, trust_remote_code=True)
model = model.quantize(4).cuda()
```

The table below compares the GPU memory usage and MMLU accuracy of models at different quantization levels:
|  Model    |  Precision    | Memory Usage (GB) | MMLU Accuracy |
| :---------: | :---------: | :------------: | :---------: |
| XVERSE-13B-Chat | BF16 / FP16 |      28.2      |    60.2     |
| XVERSE-13B-Chat |    INT8     |      16.8      |    60.3     |
| XVERSE-13B-Chat |    INT4     |      10.9      |    55.0     |

## Fine-tuning
Both XVERSE-13B and XVERSE-13B-Chat allow developers to fine-tune for improved performance. Here, we attempted to use [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) for compatible fine-tuning training with XVERSE-13B, and tested it in an environment with 8 * Nvidia A800 80 GB + deepspeed.
Below, we provide the detailed method for `full parameters fine-tuning`.


### Environment Setup

Download the LLaMA Efficient Tuning project and [install dependencies] (https://github.com/hiyouga/LLaMA-Efficient-Tuning#getting-started) as required.

### Training

Training launch script:
> Replace model_path with your own model path.

> Both XVERSE-13B and XVERSE-13B-Chat are trained based on bfloat16. It is recommended to use bfloat16 for fine-tuning training.
```bash
deepspeed --num_gpus=8 src/train_bash.py \
Â  Â  --stage sft \
Â  Â  --model_name_or_path model_path \
Â  Â  --do_train \
Â  Â  --dataset alpaca_gpt4_en \
Â  Â  --template default \
Â  Â  --finetuning_type full \
Â  Â  --output_dir output_model_path \
Â  Â  --overwrite_cache \
Â  Â  --per_device_train_batch_size 4 \
Â  Â  --per_device_eval_batch_size 4 \
Â  Â  --gradient_accumulation_steps 4 \
Â  Â  --preprocessing_num_workers 16 \
Â  Â  --lr_scheduler_type cosine \
Â  Â  --logging_steps 10 \
Â  Â  --save_steps 200 \
Â  Â  --eval_steps 200 \
Â  Â  --learning_rate 2e-5 \
Â  Â  --max_grad_norm 0.5 \
Â  Â  --num_train_epochs 2.0 \
Â  Â  --evaluation_strategy steps \
Â  Â  --load_best_model_at_end \
Â  Â  --plot_loss \
Â  Â  --bf16 \
Â  Â  --padding_side right \
Â  Â  --deepspeed deepspeed.json
```
deep_speed.json parameter settingsï¼š
```json
{
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "gradient_clipping": "auto",
    "zero_allow_untested_optimizer": true,
    "bf16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2,
        "allgather_partitions": true,
        "reduce_scatter": true,
        "overlap_comm": false,
        "contiguous_gradients": true
    }
}
```

## Limitations and Disclaimer

Like all other Large Language Models (LLMs), XVERSE-13B may produce inaccurate, biased, or otherwise offensive content under certain circumstances. Therefore, please use the content generated by the model with caution and refrain from disseminating harmful content. Before deploying any application of XVERSE-13B, developers should conduct safety tests and optimization of the model according to its specific application.

We strongly warn against the use of the XVERSE-13B model for producing or spreading harmful information, or conducting any activities that might harm the public, national, or social security, or violate regulations. We assume no responsibility for any problems arising from the use of the XVERSE-13B model, whether it be data security issues, public opinion risks, or any risks and issues caused by misunderstanding, misuse, dissemination, or non-compliance with the model.

## Open Source License

The use of the source code in this repository must follow the [Apache-2.0](LICENSE) open-source license, while the use of the model weights of XVERSE-13B needs to adhere to the [Model License Agreement](MODEL_LICENSE.pdf).

The XVERSE-13B model weights are **fully open** to academic research and support **free commercial use**. Commercial use requires an application for a commercial use license by sending an email to <opensource@xverse.cn>.

